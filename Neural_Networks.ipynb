{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data2= pd.read_csv(r'/content/attack_annotated_comments.csv')\n",
        "data3= pd.read_csv(r'/content/attack_annotations.csv')\n",
        "\n",
        "# Merge the datasets\n",
        "merged_dataset = pd.merge(data2, data3, on='rev_id')\n",
        "\n",
        "# Save the merged dataset to a new file\n",
        "merged_dataset.to_csv(r'C:\\Users\\USER\\Downloads\\merged_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "S_Oo6gbJnNE2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running code RNN\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(r'/content/C:\\Users\\USER\\Downloads\\merged_dataset.csv')\n",
        "# Preprocess the dataset\n",
        "texts = data['comment'].values\n",
        "labels = data['attack'].values\n",
        "\n",
        "# Reduce the dataset size\n",
        "max_samples = 50000  # Choose a smaller number of samples to use\n",
        "texts = texts[:max_samples]\n",
        "labels = labels[:max_samples]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_len = 200  # Set a maximum sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_len))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model.save(\"aggression_detection_model.h5\")\n"
      ],
      "metadata": {
        "id": "y0uGHxjSqetF",
        "outputId": "c1b237e6-f3c3-4aec-8b7f-2362ea260e0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 234s 366ms/step - loss: 0.3029 - accuracy: 0.8922 - val_loss: 0.2593 - val_accuracy: 0.9084\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 213s 342ms/step - loss: 0.2425 - accuracy: 0.9074 - val_loss: 0.2564 - val_accuracy: 0.9076\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 212s 339ms/step - loss: 0.2259 - accuracy: 0.9096 - val_loss: 0.2600 - val_accuracy: 0.9080\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 211s 338ms/step - loss: 0.2149 - accuracy: 0.9104 - val_loss: 0.2728 - val_accuracy: 0.9092\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 212s 340ms/step - loss: 0.2089 - accuracy: 0.9119 - val_loss: 0.2800 - val_accuracy: 0.9095\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.2800 - accuracy: 0.9095\n",
            "Test Loss: 0.2800\n",
            "Test Accuracy: 0.9095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# running CNN\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(r'/content/C:\\Users\\USER\\Downloads\\merged_dataset.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "texts = data['comment'].values\n",
        "labels = data['attack'].values\n",
        "\n",
        "# Reduce the dataset size\n",
        "max_samples = 50000  # Choose a smaller number of samples to use\n",
        "texts = texts[:max_samples]\n",
        "labels = labels[:max_samples]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_len = 200  # Set a maximum sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (max_len,)\n",
        "num_classes = 1\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_len))\n",
        "model.add(tf.keras.layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model.save(\"aggression_detection_model_CNN.h5\")\n"
      ],
      "metadata": {
        "id": "9wIVr3XdVzb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we load the dataset using pd.read_csv() and preprocess it by tokenizing the text using Tokenizer and padding the sequences using pad_sequences. We then split the data into training and testing sets using train_test_split.\n",
        "\n",
        "Next, we define the model architecture using tf.keras.Sequential and add an embedding layer, bidirectional GRU layer, and a dense layer. We compile the model with a binary cross-entropy loss and the Adam optimizer. Finally, we train the model using fit() with the training data and evaluate its performance on the testing data.\n",
        "\n",
        "Please note that you will need to update the 'path/to/wikipedia_talk_labels_personal_attacks.csv' with the actual file path to the dataset on your local machine. Also, feel free to modify the model architecture or other parameters as needed for your research."
      ],
      "metadata": {
        "id": "HLmxdGdcqkc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, after training the model using fit(), we evaluate its performance on the testing data using evaluate(). The loss and accuracy are then printed to the console. Finally, we save the trained model using save() to a file named \"aggression_detection_model.h5\". You can modify the file name or path as needed.\n",
        "\n",
        "Remember to update the 'path/to/wikipedia_talk_labels_personal_attacks.csv' with the actual file path to the dataset on your local machine. Additionally, you can modify the model architecture, training parameters, or add additional code for further analysis or experimentation."
      ],
      "metadata": {
        "id": "m6YZgxyZq3ML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the code would include the training progress and the evaluation results. During the training process, you would see the loss and accuracy values for each epoch printed on the console. After the training is completed, the code evaluates the trained model on the testing data and prints the test loss and test accuracy.\n",
        "\n",
        "Here's an example of what the output might look like:\n",
        "\n",
        "\n",
        "\n",
        "Epoch 1/5\n",
        "2500/2500 [==============================] - 10s 4ms/step - loss: 0.2523 - accuracy: 0.8972 - val_loss: 0.1798 - val_accuracy: 0.9276\n",
        "Epoch 2/5\n",
        "2500/2500 [==============================] - 10s 4ms/step - loss: 0.1447 - accuracy: 0.9443 - val_loss: 0.1712 - val_accuracy: 0.9305\n",
        "Epoch 3/5\n",
        "2500/2500 [==============================] - 10s 4ms/step - loss: 0.1165 - accuracy: 0.9563 - val_loss: 0.1765 - val_accuracy: 0.9325\n",
        "Epoch 4/5\n",
        "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0961 - accuracy: 0.9650 - val_loss: 0.1890 - val_accuracy: 0.9316\n",
        "Epoch 5/5\n",
        "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0795 - accuracy: 0.9715 - val_loss: 0.2044 - val_accuracy: 0.9312\n",
        "\n",
        "Test Loss: 0.2044\n",
        "Test Accuracy: 0.9312\n",
        "\n",
        "\n",
        "This output shows the training progress for each epoch, including the loss and accuracy values for the training and validation sets. Finally, it displays the test loss and test accuracy obtained by evaluating the trained model on the testing data"
      ],
      "metadata": {
        "id": "LjwKfCAarFmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there are alternative ways to code the aggression detection problem using the \"Wikipedia Talk Labels: Personal Attacks\" dataset. Here's an alternative approach that uses a different model architecture called a convolutional neural network (CNN):"
      ],
      "metadata": {
        "id": "NyRRnVSnrlPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial code I provided in the beginning used a recurrent neural network (RNN) model architecture, specifically a bidirectional GRU (Gated Recurrent Unit) network. Here's the relevant part of the code:\n",
        "\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_len))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "In this architecture, an embedding layer is used to represent words as dense vectors. The output of the embedding layer is then passed through a bidirectional GRU layer, which helps capture both forward and backward contextual information. Finally, a dense layer with a sigmoid activation function is added to produce a binary classification output.\n",
        "\n",
        "Both the CNN and RNN architectures can be effective for text classification tasks like aggression detection, but they have different properties and may perform differently depending on the specific dataset and problem. It is often recommended to experiment with different architectures to find the one that yields the best results for a particular task."
      ],
      "metadata": {
        "id": "Yi7pgWgUrrmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Running FNN\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(r'/content/C:\\Users\\USER\\Downloads\\merged_dataset.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "texts = data['comment'].values\n",
        "labels = data['attack'].values\n",
        "\n",
        "# Reduce the dataset size\n",
        "max_samples = 50000  # Choose a smaller number of samples to use\n",
        "texts = texts[:max_samples]\n",
        "labels = labels[:max_samples]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_len = 200  # Set a maximum sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (max_len,)\n",
        "num_classes = 1\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_len))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model.save(\"aggression_detection_model_FNN.h5\")"
      ],
      "metadata": {
        "id": "qEzUhhZWvrXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f735142-a236-43e6-988f-062487e727e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 64s 101ms/step - loss: 0.2975 - accuracy: 0.8920 - val_loss: 0.2590 - val_accuracy: 0.9055\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 57s 91ms/step - loss: 0.2356 - accuracy: 0.9071 - val_loss: 0.2723 - val_accuracy: 0.9067\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 56s 90ms/step - loss: 0.2186 - accuracy: 0.9103 - val_loss: 0.2906 - val_accuracy: 0.9087\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.2115 - accuracy: 0.9115 - val_loss: 0.3236 - val_accuracy: 0.9083\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 56s 90ms/step - loss: 0.2070 - accuracy: 0.9111 - val_loss: 0.3256 - val_accuracy: 0.9053\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.3256 - accuracy: 0.9053\n",
            "Test Loss: 0.3256\n",
            "Test Accuracy: 0.9053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Running BERT\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(r'/content/C:\\Users\\USER\\Downloads\\merged_dataset.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "texts = data['comment'].values\n",
        "labels = data['attack'].values\n",
        "\n",
        "# Reduce the dataset size\n",
        "max_samples = 50000  # Choose a smaller number of samples to use\n",
        "texts = texts[:max_samples]\n",
        "labels = labels[:max_samples]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_len = 200  # Set a maximum sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_len))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model.save(\"aggression_detection_model_BERT.h5\")\n"
      ],
      "metadata": {
        "id": "1hVWSSxeusYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4567adec-6b9b-4369-e4ea-3272692ab061"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 243s 375ms/step - loss: 0.3023 - accuracy: 0.8920 - val_loss: 0.2596 - val_accuracy: 0.9056\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 234s 374ms/step - loss: 0.2408 - accuracy: 0.9076 - val_loss: 0.2546 - val_accuracy: 0.9090\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 234s 375ms/step - loss: 0.2237 - accuracy: 0.9094 - val_loss: 0.2638 - val_accuracy: 0.9082\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 229s 367ms/step - loss: 0.2127 - accuracy: 0.9112 - val_loss: 0.2840 - val_accuracy: 0.9084\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 234s 374ms/step - loss: 0.2075 - accuracy: 0.9121 - val_loss: 0.2918 - val_accuracy: 0.9074\n",
            "313/313 [==============================] - 22s 69ms/step - loss: 0.2918 - accuracy: 0.9074\n",
            "Test Loss: 0.2918\n",
            "Test Accuracy: 0.9074\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}